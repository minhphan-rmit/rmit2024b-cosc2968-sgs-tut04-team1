{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the project is to train a model that will result in a mathematical equation. This equation will use specific input features (such as temperature, water quality, sunlight exposure, etc.) to predict the percentage of coral bleaching. The goal is for this equation to serve as a practical tool, enabling people to pinpoint factors contributing to coral bleaching and to guide them in taking steps to mitigate or prevent it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.preprocessing import OneHotEncoder      \n",
    "from sklearn.model_selection import KFold   \n",
    "from statistics import mean\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(r'.\\dataset\\global_bleaching_environmental_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____________ Dataset info ____________\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41184 entries, 0 to 41183\n",
      "Data columns (total 29 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Latitude_Degrees     41184 non-null  float64\n",
      " 1   Longitude_Degrees    41184 non-null  float64\n",
      " 2   Ocean_Name           41184 non-null  object \n",
      " 3   Realm_Name           41184 non-null  object \n",
      " 4   Ecoregion_Name       41181 non-null  object \n",
      " 5   Distance_to_Shore    41182 non-null  float64\n",
      " 6   Exposure             41184 non-null  object \n",
      " 7   Turbidity            41178 non-null  float64\n",
      " 8   Cyclone_Frequency    41184 non-null  float64\n",
      " 9   Depth_m              39387 non-null  float64\n",
      " 10  Percent_Cover        28764 non-null  float64\n",
      " 11  Bleaching_Level      41184 non-null  object \n",
      " 12  Percent_Bleaching    34407 non-null  float64\n",
      " 13  ClimSST              41073 non-null  float64\n",
      " 14  Temperature_Kelvin   41038 non-null  float64\n",
      " 15  Temperature_Mean     41054 non-null  float64\n",
      " 16  Temperature_Maximum  41054 non-null  float64\n",
      " 17  Windspeed            41057 non-null  float64\n",
      " 18  SSTA                 41038 non-null  float64\n",
      " 19  SSTA_Mean            41054 non-null  float64\n",
      " 20  SSTA_Maximum         41054 non-null  float64\n",
      " 21  SSTA_Frequency       41038 non-null  float64\n",
      " 22  SSTA_DHW             41038 non-null  float64\n",
      " 23  TSA                  41038 non-null  float64\n",
      " 24  TSA_Maximum          41054 non-null  float64\n",
      " 25  TSA_Mean             41054 non-null  float64\n",
      " 26  TSA_Frequency        41038 non-null  float64\n",
      " 27  TSA_DHW              41038 non-null  float64\n",
      " 28  Date                 41184 non-null  object \n",
      "dtypes: float64(23), object(6)\n",
      "memory usage: 9.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('\\n____________ Dataset info ____________')\n",
    "print(raw_data.info())       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle the outlier using iqr quantile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle the missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the percentage of missing value in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Missing Values  Percentage\n",
      "Latitude_Degrees                  0    0.000000\n",
      "Longitude_Degrees                 0    0.000000\n",
      "Ocean_Name                        0    0.000000\n",
      "Realm_Name                        0    0.000000\n",
      "Ecoregion_Name                    3    0.007284\n",
      "Distance_to_Shore                 2    0.004856\n",
      "Exposure                          0    0.000000\n",
      "Turbidity                         6    0.014569\n",
      "Cyclone_Frequency                 0    0.000000\n",
      "Depth_m                        1797    4.363345\n",
      "Percent_Cover                 12420   30.157343\n",
      "Bleaching_Level                   0    0.000000\n",
      "Percent_Bleaching              6777   16.455420\n",
      "ClimSST                         111    0.269522\n",
      "Temperature_Kelvin              146    0.354507\n",
      "Temperature_Mean                130    0.315657\n",
      "Temperature_Maximum             130    0.315657\n",
      "Windspeed                       127    0.308372\n",
      "SSTA                            146    0.354507\n",
      "SSTA_Mean                       130    0.315657\n",
      "SSTA_Maximum                    130    0.315657\n",
      "SSTA_Frequency                  146    0.354507\n",
      "SSTA_DHW                        146    0.354507\n",
      "TSA                             146    0.354507\n",
      "TSA_Maximum                     130    0.315657\n",
      "TSA_Mean                        130    0.315657\n",
      "TSA_Frequency                   146    0.354507\n",
      "TSA_DHW                         146    0.354507\n",
      "Date                              0    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of missing values per column\n",
    "missing_values_count = raw_data.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values per column\n",
    "total_rows = len(raw_data)\n",
    "missing_percentage = (missing_values_count / total_rows) * 100\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "missing_data_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values_count,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(missing_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the column that have the percentage of missing > 25% (Percent_cover);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_remove = 'Percent_Cover'\n",
    "# Remove the column\n",
    "raw_data = raw_data.drop(columns=[column_to_remove])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reason for this:\n",
    "1. Bias Reduction:\n",
    "High Missing Rate: If a column has a high percentage of missing values, any attempt to fill in those missing values (e.g., through imputation) can introduce significant bias. The imputed values may not accurately represent the true data, leading to unreliable models.\n",
    "\n",
    "Reduced Data Quality: Columns with a lot of missing data can degrade the quality of your dataset, as imputed values might not capture the variability and true relationships within the data.\n",
    "\n",
    "2. Simplification of the Model:\n",
    "Avoiding Overfitting: Including columns with many missing values might increase the complexity of the model, leading to overfitting. Removing such columns can simplify the model, making it more generalizable.\n",
    "\n",
    "Improving Interpretability: Fewer, more relevant features make it easier to interpret and understand the model. Columns with high missing values often contribute little to the model's predictive power.\n",
    "\n",
    "3. Efficient Use of Resources:\n",
    "Reduced Computational Load: By removing columns with a high percentage of missing values, you reduce the dimensionality of your dataset, leading to faster training and testing times. This is particularly important when working with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling the categorical_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ecoregion_Name    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the data\n",
    "raw_data = pd.read_csv(r'.\\dataset\\global_bleaching_environmental_cleaned.csv')\n",
    "\n",
    "# Categorical columns\n",
    "categorical_columns = ['Ecoregion_Name']\n",
    "\n",
    "# Impute missing values with the most frequent value (mode)\n",
    "imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "raw_data[categorical_columns] = imputer_categorical.fit_transform(raw_data[categorical_columns])\n",
    "\n",
    "# Display the result to ensure missing values are imputed\n",
    "print(raw_data[categorical_columns].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling for numerical and continuous features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth_m                0\n",
      "Percent_Bleaching      0\n",
      "ClimSST                0\n",
      "Temperature_Kelvin     0\n",
      "Temperature_Mean       0\n",
      "Temperature_Maximum    0\n",
      "Windspeed              0\n",
      "SSTA                   0\n",
      "SSTA_Mean              0\n",
      "SSTA_Maximum           0\n",
      "SSTA_Frequency         0\n",
      "SSTA_DHW               0\n",
      "TSA                    0\n",
      "TSA_Maximum            0\n",
      "TSA_Mean               0\n",
      "TSA_Frequency          0\n",
      "TSA_DHW                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Numerical columns\n",
    "numerical_columns = ['Depth_m', 'Percent_Bleaching', 'ClimSST', 'Temperature_Kelvin', \n",
    "                     'Temperature_Mean', 'Temperature_Maximum', 'Windspeed', 'SSTA', \n",
    "                     'SSTA_Mean', 'SSTA_Maximum', 'SSTA_Frequency', 'SSTA_DHW', \n",
    "                     'TSA', 'TSA_Maximum', 'TSA_Mean', 'TSA_Frequency', 'TSA_DHW']\n",
    "\n",
    "# Impute missing values with the median (or change strategy to 'mean' if appropriate)\n",
    "imputer_numerical = SimpleImputer(strategy='median')\n",
    "raw_data[numerical_columns] = imputer_numerical.fit_transform(raw_data[numerical_columns])\n",
    "\n",
    "# Display the result to ensure missing values are imputed\n",
    "print(raw_data[numerical_columns].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle the outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The way to handle the outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling outliers using the Interquartile Range (IQR) is a common and effective method. The IQR is the range between the first quartile (Q1) and the third quartile (Q3) and represents the middle 50% of the data. Outliers are typically defined as any data points that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR.\n",
    "\n",
    "Steps to Handle Outliers Using IQR:\n",
    "Calculate the IQR: Compute Q1 and Q3 for each numerical feature.\n",
    "Identify Outliers: Determine the lower and upper bounds using the IQR.\n",
    "Handle Outliers: Depending on the situation, you can:\n",
    "Remove Outliers: Drop the rows containing outliers.\n",
    "Cap Outliers: Replace outliers with the nearest value within the acceptable range (often called winsorizing).\n",
    "Transform Outliers: Apply a transformation (e.g., log) to reduce the impact of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-numeric column: Exposure\n",
      "Skipping non-numeric column: Bleaching_Level\n",
      "       Latitude_Degrees  Longitude_Degrees  Distance_to_Shore     Turbidity  \\\n",
      "count      41184.000000       41184.000000       41182.000000  41178.000000   \n",
      "mean           7.561469          34.958047        1260.287545      0.060503   \n",
      "std           15.732738         103.414234        1547.845199      0.041034   \n",
      "min          -30.262500        -179.974300           3.200000      0.000000   \n",
      "25%           -4.715150         -78.397800         124.762500      0.033500   \n",
      "50%           10.771300          96.843300         457.610000      0.052300   \n",
      "75%           20.051000         120.880400        1789.150000      0.079400   \n",
      "max           36.750000         179.964500        4285.731250      0.148250   \n",
      "\n",
      "       Cyclone_Frequency       Depth_m  Percent_Cover  Percent_Bleaching  \\\n",
      "count       41184.000000  39387.000000   28764.000000       34407.000000   \n",
      "mean           51.963563      6.881721      19.469663           4.038013   \n",
      "std             6.633943      3.958653      20.800718           5.831110   \n",
      "min            36.255000      0.000000       0.000000           0.000000   \n",
      "25%            47.940000      3.700000       0.620000           0.000000   \n",
      "50%            50.920000      6.000000      12.500000           0.250000   \n",
      "75%            55.730000     10.000000      33.120000           6.060000   \n",
      "max            67.415000     19.450000      81.870000          15.150000   \n",
      "\n",
      "            ClimSST  Temperature_Kelvin  ...          SSTA  SSTA_Mean  \\\n",
      "count  41073.000000        41038.000000  ...  41038.000000    41054.0   \n",
      "mean     299.615091          301.367380  ...      0.250766        0.0   \n",
      "std        3.285320            1.841038  ...      0.783164        0.0   \n",
      "min      293.360000          296.670000  ...     -1.750000        0.0   \n",
      "25%      298.550000          300.270000  ...     -0.250000        0.0   \n",
      "50%      300.800000          301.650000  ...      0.240000        0.0   \n",
      "75%      302.010000          302.670000  ...      0.750000        0.0   \n",
      "max      307.200000          306.270000  ...      2.250000        0.0   \n",
      "\n",
      "       SSTA_Maximum  SSTA_Frequency      SSTA_DHW           TSA   TSA_Maximum  \\\n",
      "count  41054.000000    41038.000000  41038.000000  41038.000000  41054.000000   \n",
      "mean       3.337554        7.369851      2.639105     -0.942258      2.709119   \n",
      "std        0.727044        5.813964      3.151930      1.499553      0.629832   \n",
      "min        1.500000        0.000000      0.000000     -4.675000      1.050000   \n",
      "25%        2.820000        3.000000      0.000000     -1.810000      2.250000   \n",
      "50%        3.180000        6.000000      1.300000     -0.740000      2.560000   \n",
      "75%        3.700000       11.000000      4.170000      0.100000      3.050000   \n",
      "max        5.020000       23.000000     10.425000      2.965000      4.250000   \n",
      "\n",
      "           TSA_Mean  TSA_Frequency       TSA_DHW  \n",
      "count  41054.000000   41038.000000  41038.000000  \n",
      "mean      -1.867497       1.829758      0.648423  \n",
      "std        1.030819       2.064594      1.070589  \n",
      "min       -4.670000       0.000000      0.000000  \n",
      "25%       -2.600000       0.000000      0.000000  \n",
      "50%       -1.490000       1.000000      0.000000  \n",
      "75%       -1.220000       3.000000      1.150000  \n",
      "max        0.000000       7.500000      2.875000  \n",
      "\n",
      "[8 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "raw_data = pd.read_csv(r'.\\dataset\\global_bleaching_environmental_cleaned.csv')\n",
    "\n",
    "# List of actual numerical columns\n",
    "numerical_columns = [\n",
    "    'Latitude_Degrees', 'Longitude_Degrees', 'Distance_to_Shore', 'Exposure', \n",
    "    'Turbidity', 'Cyclone_Frequency', 'Depth_m', 'Percent_Cover', 'Bleaching_Level', \n",
    "    'Percent_Bleaching', 'ClimSST', 'Temperature_Kelvin', 'Temperature_Mean', \n",
    "    'Temperature_Maximum', 'Windspeed', 'SSTA', 'SSTA_Mean', 'SSTA_Maximum', \n",
    "    'SSTA_Frequency', 'SSTA_DHW', 'TSA', 'TSA_Maximum', 'TSA_Mean', \n",
    "    'TSA_Frequency', 'TSA_DHW'\n",
    "]\n",
    "\n",
    "# Function to calculate IQR and handle outliers\n",
    "def handle_outliers_iqr(df, columns):\n",
    "    for col in columns:\n",
    "        # Check if the column is numeric\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Option 1: Remove outliers\n",
    "            # df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "            \n",
    "            # Option 2: Cap outliers\n",
    "            df[col] = df[col].apply(lambda x: upper_bound if x > upper_bound else lower_bound if x < lower_bound else x)\n",
    "        else:\n",
    "            print(f\"Skipping non-numeric column: {col}\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Apply the function to handle outliers\n",
    "raw_data = handle_outliers_iqr(raw_data, numerical_columns)\n",
    "\n",
    "# Display the result to ensure outliers are handled\n",
    "print(raw_data.describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
