{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the project is to train a model that will result in a mathematical equation. This equation will use specific input features (such as temperature, water quality, sunlight exposure, etc.) to predict the percentage of coral bleaching. The goal is for this equation to serve as a practical tool, enabling people to pinpoint factors contributing to coral bleaching and to guide them in taking steps to mitigate or prevent it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.preprocessing import OneHotEncoder      \n",
    "from sklearn.model_selection import KFold   \n",
    "from statistics import mean\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(r'.\\dataset\\global_bleaching_environmental_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the percentage of missing value in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Reasons to Remove Columns with High Missing Value Percentage\n",
    "\n",
    "#### Data Integrity\n",
    "\n",
    "- **High Proportion of Missing Data:**\n",
    "  - If a column has 25% or more missing values, it indicates a significant portion of the data is unavailable. This can undermine the reliability of any analysis or modeling performed using that column.\n",
    "  \n",
    "- **Incomplete Information:**\n",
    "  - With such a high proportion of missing data, the column may not provide a complete or representative view of the feature, potentially leading to biased or incomplete insights.\n",
    "\n",
    "#### Impact on Model Performance\n",
    "\n",
    "- **Imputation Uncertainty:**\n",
    "  - Imputing missing values in columns with such high proportions can introduce substantial uncertainty and may not accurately represent the underlying data distribution.\n",
    "\n",
    "- **Model Complexity:**\n",
    "  - Including columns with high missing value percentages can complicate the model and may lead to overfitting if imputation methods are not carefully chosen.\n",
    "\n",
    "#### Statistical Validity\n",
    "\n",
    "- **Bias in Imputation:**\n",
    "  - Imputation methods (mean, median, mode) may not be effective or appropriate for columns with a high percentage of missing values, potentially introducing bias or reducing the validity of the statistical analysis.\n",
    "\n",
    "#### Data Quality\n",
    "\n",
    "- **Noise and Error:**\n",
    "  - Columns with extensive missing data may be indicative of data quality issues or errors in data collection processes, leading to unreliable or noisy datasets.\n",
    "\n",
    "#### Simplifying the Dataset\n",
    "\n",
    "- **Focus on Relevant Features:**\n",
    "  - Removing columns with excessive missing values helps in focusing on more relevant and reliable features, simplifying the dataset and potentially improving model performance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Missing Values  Percentage\n",
      "Latitude_Degrees                  0    0.000000\n",
      "Longitude_Degrees                 0    0.000000\n",
      "Ocean_Name                        0    0.000000\n",
      "Realm_Name                        0    0.000000\n",
      "Ecoregion_Name                    3    0.007284\n",
      "Distance_to_Shore                 2    0.004856\n",
      "Exposure                          0    0.000000\n",
      "Turbidity                         6    0.014569\n",
      "Cyclone_Frequency                 0    0.000000\n",
      "Depth_m                        1797    4.363345\n",
      "Percent_Cover                 12420   30.157343\n",
      "Bleaching_Level                   0    0.000000\n",
      "Percent_Bleaching              6777   16.455420\n",
      "ClimSST                         111    0.269522\n",
      "Temperature_Kelvin              146    0.354507\n",
      "Temperature_Mean                130    0.315657\n",
      "Temperature_Maximum             130    0.315657\n",
      "Windspeed                       127    0.308372\n",
      "SSTA                            146    0.354507\n",
      "SSTA_Mean                       130    0.315657\n",
      "SSTA_Maximum                    130    0.315657\n",
      "SSTA_Frequency                  146    0.354507\n",
      "SSTA_DHW                        146    0.354507\n",
      "TSA                             146    0.354507\n",
      "TSA_Maximum                     130    0.315657\n",
      "TSA_Mean                        130    0.315657\n",
      "TSA_Frequency                   146    0.354507\n",
      "TSA_DHW                         146    0.354507\n",
      "Date                              0    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of missing values per column\n",
    "missing_values_count = raw_data.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values per column\n",
    "total_rows = len(raw_data)\n",
    "missing_percentage = (missing_values_count / total_rows) * 100\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "missing_data_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values_count,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(missing_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the column that have the percentage of missing > 25% (Percent_cover);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_remove = 'Percent_Cover'\n",
    "# Remove the column\n",
    "raw_data = raw_data.drop(columns=[column_to_remove])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reason for this:\n",
    "1. Bias Reduction:\n",
    "High Missing Rate: If a column has a high percentage of missing values, any attempt to fill in those missing values (e.g., through imputation) can introduce significant bias. The imputed values may not accurately represent the true data, leading to unreliable models.\n",
    "\n",
    "Reduced Data Quality: Columns with a lot of missing data can degrade the quality of your dataset, as imputed values might not capture the variability and true relationships within the data.\n",
    "\n",
    "2. Simplification of the Model:\n",
    "Avoiding Overfitting: Including columns with many missing values might increase the complexity of the model, leading to overfitting. Removing such columns can simplify the model, making it more generalizable.\n",
    "\n",
    "Improving Interpretability: Fewer, more relevant features make it easier to interpret and understand the model. Columns with high missing values often contribute little to the model's predictive power.\n",
    "\n",
    "3. Efficient Use of Resources:\n",
    "Reduced Computational Load: By removing columns with a high percentage of missing values, you reduce the dimensionality of your dataset, leading to faster training and testing times. This is particularly important when working with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle the outliers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Outliers Using the Interquartile Range (IQR)\n",
    "\n",
    "Handling outliers using the Interquartile Range (IQR) is a common and effective method. The IQR is the range between the first quartile (Q1) and the third quartile (Q3) and represents the middle 50% of the data. \n",
    "\n",
    "#### Steps to Handle Outliers Using IQR\n",
    "\n",
    "1. **Calculate the IQR:**\n",
    "   - Compute Q1 (the first quartile) and Q3 (the third quartile) for each numerical feature.\n",
    "   \n",
    "2. **Identify Outliers:**\n",
    "   - Determine the lower and upper bounds using the IQR:\n",
    "     - **Lower Bound:** $$ \\text{Lower Bound} = Q1 - 1.5 \\times \\text{IQR} $$\n",
    "     - **Upper Bound:** $$ \\text{Upper Bound} = Q3 + 1.5 \\times \\text{IQR} $$\n",
    "\n",
    "3. **Handle Outliers:**\n",
    "   - Depending on the situation, you can choose from the following methods:\n",
    "     - **Remove Outliers:** Drop the rows containing outliers.\n",
    "     - **Cap Outliers:** Replace outliers with the nearest value within the acceptable range (often called winsorizing).\n",
    "     - **Transform Outliers:** Apply a transformation (e.g., log) to reduce the impact of outliers.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Imputation with the Mode\n",
    "\n",
    "Imputation with the mode involves replacing missing values in a dataset with the most frequent value (the mode) from the column. This method is commonly used for categorical data, where replacing missing values with the most common category is often a sensible approach.\n",
    "\n",
    "#### How Imputation with Mode Works:\n",
    "\n",
    "1. **Identify the Mode:**\n",
    "   - **Definition:** The mode is the value that appears most frequently in a column.\n",
    "     - For **categorical data**, this is the most common category.\n",
    "     - For **numerical data**, the mode is the number that occurs most frequently. However, this method is less commonly used for numerical data compared to mean or median imputation.\n",
    "\n",
    "2. **Replace Missing Values:**\n",
    "   - **Imputation Process:** Missing values in the column are replaced with the mode. This ensures that the dataset remains complete and the imputed values reflect the most common observed values in that column.\n",
    "\n",
    "---\n",
    "\n",
    "#### Imputation Using the Mean\n",
    "\n",
    "Imputation using the mean value is a common approach for numerical data. This method involves replacing missing values with the mean (average) of the observed values in a column. Itâ€™s particularly useful for datasets where missing values are missing at random and the data is approximately normally distributed.\n",
    "\n",
    "#### How Imputation with Mean Works:\n",
    "\n",
    "1. **Calculate the Mean:**\n",
    "   - **Definition:** Compute the mean of the column, excluding the missing values.\n",
    "\n",
    "2. **Replace Missing Values:**\n",
    "   - **Imputation Process:** Substitute the missing values with the calculated mean. This helps maintain the overall statistical properties of the dataset.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imputed and outliers removed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the data\n",
    "raw_data = pd.read_csv(r'.\\dataset\\global_bleaching_environmental_cleaned.csv')\n",
    "\n",
    "# Categorical columns\n",
    "categorical_columns = ['Ecoregion_Name']\n",
    "\n",
    "# Impute missing values in categorical columns with the most frequent value (mode)\n",
    "imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "raw_data[categorical_columns] = imputer_categorical.fit_transform(raw_data[categorical_columns])\n",
    "\n",
    "# Numerical columns\n",
    "numerical_columns = ['Depth_m', 'Percent_Bleaching', 'ClimSST', 'Temperature_Kelvin', \n",
    "                     'Temperature_Mean', 'Temperature_Maximum', 'Windspeed', 'SSTA', \n",
    "                     'SSTA_Mean', 'SSTA_Maximum', 'SSTA_Frequency', 'SSTA_DHW', \n",
    "                     'TSA', 'TSA_Maximum', 'TSA_Mean', 'TSA_Frequency', 'TSA_DHW']\n",
    "\n",
    "# Impute missing values in numerical columns with the median\n",
    "imputer_numerical = SimpleImputer(strategy='median')\n",
    "raw_data[numerical_columns] = imputer_numerical.fit_transform(raw_data[numerical_columns])\n",
    "\n",
    "# Function to calculate IQR and handle outliers\n",
    "def handle_outliers_iqr(df, columns):\n",
    "    for col in columns:\n",
    "        # Check if the column is numeric\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Remove outliers by filtering rows\n",
    "            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "        else:\n",
    "            print(f\"Skipping non-numeric column: {col}\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Apply the function to handle outliers\n",
    "raw_data = handle_outliers_iqr(raw_data, numerical_columns)\n",
    "\n",
    "# Save the updated DataFrame to a new file\n",
    "raw_data.to_csv(r'.\\dataset\\global_bleaching_environmental_cleaned_imputed_no_outliers.csv', index=False)\n",
    "\n",
    "print(\"Data imputed and outliers removed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
