{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration\n",
    "PEARSON --> continous\n",
    "cramer/phi-k --> non nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text\n",
    "'Country_Name','State_Island_Province_Name','City_Town_Name','Site_Name','Ecoregion_Name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unsupported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Distance_to_Shore','Turbidity','Percent_Bleaching'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Exposure','Substrate_Name'\n",
    "'Data_Source','Ocean_Name','Realm_Name','Exposure',\n",
    "'Windspeed'\n",
    "#remove\n",
    "'Bleaching_Level', 'SSTA_Mean'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Latitude_Degrees\t','Longitude_Degrees\t', \n",
    "'Cyclone_Frequency','Date_Day','Date_Month','Date_Year'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fea = ['Cyclone_Frequency','Date_Day', 'Date_Month', 'Date_Year',\n",
    "'Percent_Cover','Depth_m','ClimSST',\n",
    "'Temperature_Kelvin','Temperature_Mean','Temperature_Minimum','Temperature_Maximum',\n",
    "'Temperature_Kelvin_Standard_Deviation',\n",
    "'Windspeed',\n",
    "'SSTA','SSTA_Standard_Deviation', 'SSTA_Minimum','SSTA_Maximum','SSTA_Frequency',\n",
    "'SSTA_Frequency_Standard_Deviation','SSTA_FrequencyMax','SSTA_FrequencyMean',\n",
    "'SSTA_DHW','SSTA_DHW_Standard_Deviation','SSTA_DHWMax','SSTA_DHWMean',\n",
    "'TSA','TSA_Standard_Deviation','TSA_Minimum','TSA_Maximum','TSA_Mean',\n",
    "'TSA_Frequency','TSA_Frequency_Standard_Deviation','TSA_FrequencyMax','TSA_FrequencyMean',\n",
    "'TSA_DHW','TSA_DHW_Standard_Deviation','TSA_DHWMax','TSA_DHWMean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple impute\n",
    "'Distance_to_Shore ','Turbidity','Percent_Cover','ClimSST','Deph_m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert to num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Percent_Cover','Depth_m','ClimSST','Temperature_Kelvin','Temperature_Mean','Temperature_Minimum','Temperature_Maximum'\n",
    "'Temperature_Kelvin_Standard_Deviation'\n",
    "'Windspeed'\n",
    "'SSTA','SSTA_Standard_Deviation','SSTA_Mean','SSTA_Minimum','SSTA_Maximum','SSTA_Frequency'\n",
    "'SSTA_Frequency_Standard_Deviation','SSTA_FrequencyMax','SSTA_FrequencyMean'\n",
    "'SSTA_DHW','SSTA_DHW_Standard_Deviation','SSTA_DHWMax','SSTA_DHWMean',\n",
    "'TSA','TSA_Standard_Deviation','TSA_Minimum','TSA_Maximum','TSA_Mean',\n",
    "'TSA_Frequency','TSA_Frequency_Standard_Deviation','TSA_FrequencyMax','TSA_FrequencyMean'\n",
    "'TSA_DHW','TSA_DHW_Standard_Deviation','TSA_DHWMax','TSA_DHWMean'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = dataset['Percent_Cover'] != 'nd'\n",
    "mean = dataset[condition]['Percent_Cover'].mean()\n",
    "dataset['Percent_Cover'] = dataset['Percent_Cover'].replace('nd', mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "onehotencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_TOTAL_NO_OF_ROOMS = True): # MUST NO *args or **kargs\n",
    "        self.add_TOTAL_NO_OF_ROOMS = add_TOTAL_NO_OF_ROOMS\n",
    "    def fit(self, feature_values, labels = None):\n",
    "        return self  # nothing to do here\n",
    "    def transform(self, feature_values, labels = None):\n",
    "        if self.add_TOTAL_NO_OF_ROOMS:        \n",
    "            NO_OF_ROOMS_id, NO_OF_TOILETS_id = 1, 2 # column indices in num_feat_names. can't use column names b/c the transformer SimpleImputer removed them\n",
    "            # NOTE: a transformer in a pipeline ALWAYS return dataframe.values (ie., NO header and row index)\n",
    "            \n",
    "            TOTAL_NO_OF_ROOMS = feature_values[:, NO_OF_ROOMS_id] + feature_values[:, NO_OF_TOILETS_id]\n",
    "            feature_values = np.c_[feature_values, TOTAL_NO_OF_ROOMS] #concatenate np arrays\n",
    "        return feature_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False).set_output(transform='pandas')\n",
    "ohetransform = ohe.fit_transform(dataset[['Exposure','Substrate_Name', 'Data_Source','Ocean_Name','Realm_Name']])\n",
    "ohetransform    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fuse into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset,ohetransform], axis=1).drop(columns=cat_fea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will handle missing data for these following numeric features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['Latitude_Degrees', 'Longitude_Degrees', 'Distance_to_Shore',\n",
    "       'Turbidity', 'Cyclone_Frequency', 'Date_Day', 'Date_Month', 'Date_Year',\n",
    "       'Percent_Cover', 'ClimSST', 'Temperature_Kelvin',\n",
    "       'Temperature_Mean', 'Temperature_Minimum', 'Temperature_Maximum',\n",
    "       'Temperature_Kelvin_Standard_Deviation', 'Windspeed', 'SSTA',\n",
    "       'SSTA_Standard_Deviation', 'SSTA_Minimum', 'SSTA_Maximum',\n",
    "       'SSTA_Frequency', 'SSTA_Frequency_Standard_Deviation',\n",
    "       'SSTA_FrequencyMax', 'SSTA_FrequencyMean', 'SSTA_DHW',\n",
    "       'SSTA_DHW_Standard_Deviation', 'SSTA_DHWMax', 'SSTA_DHWMean', 'TSA',\n",
    "       'TSA_Standard_Deviation', 'TSA_Minimum', 'TSA_Maximum', 'TSA_Mean',\n",
    "       'TSA_Frequency', 'TSA_Frequency_Standard_Deviation', 'TSA_FrequencyMax',\n",
    "       'TSA_FrequencyMean', 'TSA_DHW', 'TSA_DHW_Standard_Deviation',\n",
    "       'TSA_DHWMax', 'TSA_DHWMean', 'Depth_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(train_set_labels)\n",
    "print(mean)\n",
    "train_set_labels = np.where(np.nan, train_set_labels, mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the figure to a Bytesting\n",
    "\n",
    "canvas = agg.FigureCanvasAgg(fig)\n",
    "canvas.draw()\n",
    "renderer = canvas.get_renderer()\n",
    "raw_data = renderer.tostring_rgb()\n",
    "width, height = fig.get_size_inches()*fig.dpi\n",
    "\n",
    "#convert Bytestring to Image\n",
    "Image = PIL.Image.frombytes(\"RGB\", (width, height), raw_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_IQR = outliers_IQR(dataset, dataset.select_dtypes(\"number\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle outliers with z-score\n",
    "\n",
    "\n",
    "outlier = []\n",
    "def outliers_Z(data):\n",
    "    threshold = 3\n",
    "    count = 0\n",
    "\n",
    "    for i in data:\n",
    "        mean = np.mean(data[i]) \n",
    "        std = np.std(data[i])\n",
    "        for j in data[i]:\n",
    "         \n",
    "            z = (j-mean)/std \n",
    "            if np.abs(z) > threshold: \n",
    "                # notice outliers and remove it\n",
    "                data = data[data[i] != j]\n",
    "                count+=1\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_after_outliers = outliers_Z(dataset.select_dtypes(\"number\"))\n",
    "dataset1 = dataset.update(dataset_after_outliers)\n",
    "dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset.copy(deep=True)\n",
    "dataset1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimImp = ['Distance_to_Shore','Turbidity','Percent_Cover','ClimSST','Deph_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('selector', ColumnSelector(SimImp)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"median\", copy=True)) # copy=False: imputation will be done in-place \n",
    "    ])  \n",
    "      ('imputer', KNNImputer(missing_values=np.nan, n_neighbors=2)), # copy=False: imputation will be done in-place \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "    def fit(self, dataframe, labels=None):\n",
    "        return self\n",
    "    def transform(self, dataframe):\n",
    "        return dataframe[self.feature_names].values     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    ('selector', ColumnSelector(cat_fea)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value = \"NO INFO\", copy=True)), # complete missing values. copy=False: imputation will be done in-place \n",
    "    ('cat_encoder', OneHotEncoder()) # convert categorical data into one-hot vectors\n",
    "    ])\n",
    "trans_fea = cat_pipeline.fit_transform(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('selector', ColumnSelector(num_cols)),\n",
    "    ('imputer', SimpleImputer(missing_values = np.nan, strategy=\"median\", copy=False)) # copy=False: imputation will be done in-place \n",
    "    ])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline) ])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_set_val = full_pipeline.fit_transform(train_set)\n",
    "print(processed_train_set_val.shape)\n",
    "\n",
    "print('\\n____________ Processed feature values ____________')\n",
    "print(processed_train_set_val[[0, 1, 2],:].toarray())\n",
    "\n",
    "print('We have %d numeric feature + cols of onehotvector for categorical features.' %(len(num_for_missing)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
